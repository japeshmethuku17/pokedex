# -*- coding: utf-8 -*-
"""Xception_model_notebook_pokemon

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RFZDOgfeZnp2rvyDm0xWYpMpKpmei9Vn

## **Import libraries and packages**
"""

# %tensorflow_version 2.x

# import tensorflow and tensorflow.keras
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import backend as K

# import ImageDataGenerator and the related functions required for processing images
from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img
from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D

# import optimizers
from tensorflow.keras.optimizers import SGD, Adam, Adagrad, Adadelta, RMSprop

# import statements for building and loading the model
from tensorflow.keras import models
from tensorflow.keras.models import Sequential, Model, load_model
from tensorflow.keras.models import model_from_json

# import statements for callbacks
from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping

# import statements for initlializers and regularizers
from tensorflow.keras.initializers import glorot_uniform
from tensorflow.keras.regularizers import l2

# import statements for one-hot encoding, model plotting
from tensorflow.keras.utils import to_categorical, plot_model

# import statements for loading ResNet50 from keras
from tensorflow.keras.applications import resnet50, xception
#from tensorflow.keras.applications.resnet50 import ResNet50
#from tensorflow.keras.applications.resnet50 import preprocess_input

# import statements for scikit-learn
import sklearn.metrics as metrics
from sklearn.preprocessing import LabelBinarizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

from tensorflow.keras.layers.experimental import preprocessing

# import scipy
import scipy.misc

# import os for file access
import os 

# import glob
import glob

# import numpy, pandas
import numpy as np
import pandas as pd

# import opencv
import cv2

# import matplotlib
import matplotlib.pyplot as plt
from matplotlib.pyplot import imshow

# import zipfile for unzipping the data
#import zipfile

# import csv to access the csv files
import csv

# import drive to access the data from GDrive
#from google.colab import drive

# import seaborn
import seaborn as sns

# import time
from time import time

pip freeze > requirement.txt

from PIL import ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True

"""## **Load the dataset**"""

drive.mount('/content/drive')

zip_ref = zipfile.ZipFile("/content/drive/MyDrive/dataset.zip", 'r')
zip_ref.extractall("/tmp")
zip_ref.close()

train_dir = '/tmp/selected/train/'
val_dir = '/tmp/selected/validation/'
#test_dir = '/tmp/ChIA Images_6/test/'

"""## **Data Augmentation**"""

training_batch_size = 16
validation_batch_size = 16
def get_images(img_size, preprocessing):

  
  train_datagen = ImageDataGenerator(rotation_range=20,
                              horizontal_flip= True,
                              height_shift_range=0.1,
                              width_shift_range=0.1,
                              shear_range=0.2,
                              zoom_range=0.2,
                              fill_mode="nearest",
                              #rescale=1./255,
                              preprocessing_function = preprocessing)
  
  datagen = ImageDataGenerator(#rescale=1./255,
                               preprocessing_function= preprocessing)
  
  train_datagen = train_datagen.flow_from_directory(train_dir,
                                            batch_size= training_batch_size,
                                            shuffle= True,
                                            target_size = (img_size, img_size),
                                            class_mode = 'categorical')
  
  val_datagen = datagen.flow_from_directory(val_dir,
                                            batch_size= validation_batch_size,
                                            shuffle= False,
                                            target_size = (img_size, img_size),
                                            class_mode = 'categorical')
  
  
  
  return train_datagen, val_datagen

train_datagen, val_datagen = get_images(299, preprocessing= xception.preprocess_input)

def visualize_training(history, epochs):
  fig = plt.figure()
  
  #plt.subplot(1,2,1)
  plt.plot(history.history['accuracy']);
  plt.plot(history.history['val_accuracy'])
  plt.title("Model Accuracy")
  plt.ylabel('accuracy')
  plt.xlabel('epoch')
  #plt.figsize()
  label_1 = "{:.2f}".format(history.history['accuracy'][-1])
  label_2 = "{:.2f}".format(history.history['val_accuracy'][-1])
  plt.annotate(label_1, (epochs-1,history.history['accuracy'][-1]), textcoords="offset points", 
               xytext=(0,10),ha='center')
  plt.annotate(label_2, (epochs-1,history.history['val_accuracy'][-1]), textcoords="offset points", 
               xytext=(0,10),ha='center')
  plt.legend(['training', 'validation'], loc='upper left')

  fig = plt.figure()

  #plt.subplot(1,2,2)
  plt.plot(history.history['loss']);
  plt.plot(history.history['val_loss'])
  plt.title("Model Loss")
  plt.ylabel('loss')
  plt.xlabel('epoch')
  label_3 = "{:.2f}".format(history.history['loss'][-1])
  label_4 = "{:.2f}".format(history.history['val_loss'][-1])
  plt.annotate(label_3, (epochs-1,history.history['loss'][-1]), textcoords="offset points", 
               xytext=(0,10),ha='center')
  plt.annotate(label_4, (epochs-1,history.history['val_loss'][-1]), textcoords="offset points", 
               xytext=(0,10),ha='center')
  plt.legend(['training', 'validation'], loc='upper left')

  plt.show()

def print_classification_report(model, data_gen, batch_size):
  true_labels = data_gen.labels
  data_gen.reset()
  predicted_labels = model.predict(data_gen, steps = np.ceil(len(true_labels)/batch_size))
  #print(list( np.argmax(a) for a in predicted_labels), true_labels)
  print(classification_report(true_labels, list( np.argmax(a) for a in predicted_labels)))
  print("Confusion Matrix:\n ",confusion_matrix(true_labels,list( np.argmax(a) for a in predicted_labels)))

"""## **Build the model**"""

xception_base = xception.Xception(weights='imagenet', include_top= False, input_shape= (299, 299, 3))

xception_base.summary()

model = models.Sequential()
model.add(xception_base)
model.add(keras.layers.GlobalAveragePooling2D())
model.add(keras.layers.Dropout(0.3))
model.add(keras.layers.Dense(512, activation='relu'))
# model.add(keras.layers.Dropout(0.4))
# model.add(keras.layers.Dense(512, activation='relu'))
# model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dropout(0.4))
model.add(keras.layers.Dense(256, activation='relu'))
model.add(keras.layers.Dense(7, activation= 'softmax'))
model.summary()

xception_base.trainable = False
xception_base.training = False

model.summary()

"""## **Compile the model**"""

lr = 1e-3
model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy'])

callbacks_list = [keras.callbacks.ModelCheckpoint(
        filepath = '/content/xception-model.h5',
        monitor = 'val_accuracy',
        verbose=1,
        save_best_only = True),
        keras.callbacks.EarlyStopping(
            monitor= 'val_accuracy',
            patience = 6,
            mode = 'max',
            verbose= 1
        )]

"""## **Train the model**"""

n_epochs = 2
history = model.fit(train_datagen,
                             steps_per_epoch=len(train_datagen.filenames)//training_batch_size,
                             epochs=n_epochs,
                             validation_data=val_datagen,
                             validation_steps = int(np.ceil(len(val_datagen.filenames)/validation_batch_size)),
                             callbacks=callbacks_list,
                             verbose=1)

"""## **Fine tune the model**"""

xception_base.trainable = True
set_trainable = False
for layer in xception_base.layers[:126]:
  set_trainable = False
  if set_trainable:
    layer.trainable = True
  else:
    layer.trainable= False

for layer in xception_base.layers[126:]:
  if isinstance(layer, keras.layers.BatchNormalization):
    layer.trainable = False

for i, layer in enumerate(xception_base.layers):
  print(i, layer.name, layer.trainable)

model.summary()

lr = 1e-4
model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy'])

n_epochs = 40
history = model.fit(train_datagen,
                             steps_per_epoch=len(train_datagen.filenames)//training_batch_size,
                             epochs=n_epochs,
                             validation_data=val_datagen,
                             validation_steps=int(np.ceil(len(val_datagen.filenames)/validation_batch_size)),
                             callbacks=callbacks_list,
                             verbose=1)

POKEMON_SAVED_MODEL = "/content/drive/MyDrive/exp_saved_model"
tf.saved_model.save(model, POKEMON_SAVED_MODEL)

"""## **Visualize the training process**"""

visualize_training(history, n_epochs)

"""## **Load the saved model**"""

loaded_model = load_model('/content/xception-model.h5')

loaded_model.summary()

"""## **Evaluate the model**"""

_, evaluate_datagen = get_images(299, preprocessing= xception.preprocess_input)

model_result = loaded_model.evaluate(
    evaluate_datagen,
    steps= None)

"""## **Classification Report**"""

print_classification_report(loaded_model, evaluate_datagen,  validation_batch_size)